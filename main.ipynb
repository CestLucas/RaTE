{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /Users/gao/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to /Users/gao/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# from transformers import BertTokenizer, BertForMaskedLM, DistilBertForMaskedLM\n",
    "from transformers import BertTokenizer, BertForMaskedLM\n",
    "from transformers import pipeline\n",
    "import nltk\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import torch"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-11T21:46:17.100493Z",
     "start_time": "2023-06-11T21:46:14.541121Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "outputs": [],
   "source": [
    "# parameters\n",
    "top_k = 10\n",
    "eval_taxo_path = \"taxos/HiExpan1.txt\"\n",
    "use_queries = \"custom_queries/default_queries.txt\""
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-12T01:43:10.189950Z",
     "start_time": "2023-06-12T01:43:10.187122Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('{token} [MASK]', '{token} [MASK]'), ('[MASK] {token}', '[MASK] {token}'), ('{token} is a [MASK]', 'is a'), ('{token} is an [MASK]', 'is an'), ('{token} is a kind of [MASK]', 'kind of'), ('{token} is a type of [MASK]', 'type of'), ('{token} is an example of [MASK]', 'example of'), ('[MASK] such as {token}', 'such as'), ('A [MASK] such as {token}', 'a such as'), ('An [MASK] such as {token}', 'an such as'), ('My favorite [MASK] is {token}', 'favorite')]\n"
     ]
    }
   ],
   "source": [
    "query_templates = []\n",
    "column_names = []\n",
    "\n",
    "with open(use_queries, \"r\") as fin:\n",
    "    for line in fin.readlines():\n",
    "        if not line.startswith(\"#\"):\n",
    "            query, name = line.split(\",\")\n",
    "            query_templates.append(query.strip())\n",
    "            column_names.append(name.strip())\n",
    "\n",
    "print(list(zip(query_templates, column_names)))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-12T01:50:26.460776Z",
     "start_time": "2023-06-12T01:50:26.448209Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [],
   "source": [
    "eval_parents = []\n",
    "eval_children = []\n",
    "\n",
    "with open(eval_taxo_path , \"r\") as fin:\n",
    "    for concept_pair in fin.readlines():\n",
    "        parent, child = concept_pair.split(',')\n",
    "        eval_parents.append(parent.strip())\n",
    "        eval_children.append(child.strip())"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-11T23:40:54.571762Z",
     "start_time": "2023-06-11T23:40:54.560361Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "data": {
      "text/plain": "Downloading (…)solve/main/vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "65d1cbda33e34e1facbf12c11abeb602"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Downloading (…)okenizer_config.json:   0%|          | 0.00/28.0 [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "03f6cbc8ded346938f5de67432ce4fa5"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Downloading (…)lve/main/config.json:   0%|          | 0.00/570 [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "a7c193db0e5c44da8c0c507a7b749d92"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "31"
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# instantiate a standard bert-base tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "# instantiate another tokenizer for adding customized tokens\n",
    "tokenizer_extended = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "not_in_bert_parent_lemmas = []\n",
    "\n",
    "with open(\"model_configuration/add_tokenizer_vocabulary.txt\", \"r\") as fin:\n",
    "    for word in fin.readlines():\n",
    "        not_in_bert_parent_lemmas.append(word.strip())\n",
    "\n",
    "tokenizer_extended.add_tokens(not_in_bert_parent_lemmas)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-11T23:04:04.413706Z",
     "start_time": "2023-06-11T23:04:03.829136Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "# can also use mps on mac\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "elif torch.backends.mps.is_available() and torch.backends.mps.is_built():\n",
    "    device = torch.device('mps')\n",
    "else:\n",
    "    torch.device('cpu')\n",
    "\n",
    "# # load models\n",
    "# model1a = BertForMaskedLM.from_pretrained('/content/drive/MyDrive/mlm_final_exps/entity_masking_train_test/checkpoint-117350')\n",
    "# model1a.to(device)\n",
    "# model1a.eval()\n",
    "#\n",
    "# model1b = BertForMaskedLM.from_pretrained('/content/drive/MyDrive/mlm_final_exps/entity_masking_train_test_only_one/final/checkpoint-58675')\n",
    "# model1b.to(device)\n",
    "# model1b.eval()\n",
    "#\n",
    "# model2a = BertForMaskedLM.from_pretrained('/content/drive/MyDrive/mlm_final_exps/random_masking_train_test/final/checkpoint-58675')\n",
    "# model2a.to(device)\n",
    "# model2a.eval()\n",
    "#\n",
    "# model2b = BertForMaskedLM.from_pretrained('/content/drive/MyDrive/mlm_final_exps/random_masking_train_only/checkpoint-82146')\n",
    "# model2b.to(device)\n",
    "# model2b.eval()\n",
    "#\n",
    "# model0a = BertForMaskedLM.from_pretrained('/content/drive/MyDrive/exp/yelp/out_random_masking-bert/checkpoint-80000')\n",
    "# model0a.to(device)\n",
    "# model0a.eval()\n",
    "#\n",
    "# model0b = DistilBertForMaskedLM.from_pretrained('/content/drive/MyDrive/exp/yelp/out-entity-masking/checkpoint-73675')\n",
    "# model0b.to(device)\n",
    "# model0b.eval()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-11T23:20:00.156408Z",
     "start_time": "2023-06-11T23:20:00.152038Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "data": {
      "text/plain": "Downloading pytorch_model.bin:   0%|          | 0.00/440M [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "7e6bb9f30c454174a7d2994e919fdeba"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForMaskedLM: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "data": {
      "text/plain": "Downloading (…)/main/tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "c020a8bb786f4660a435a66a39eb91d8"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# or, just use pipeline\n",
    "# !pip install transformers\n",
    "# load a single model or multiple\n",
    "#\n",
    "# unmasker1a = pipeline('fill-mask', tokenizer=tokenizer_extended, model=model1a, device=device)\n",
    "# unmasker1b = pipeline('fill-mask', tokenizer=tokenizer_extended, model=model1b, device=device)\n",
    "# unmasker2a = pipeline('fill-mask', tokenizer=tokenizer_extended, model=model2a, device=device)\n",
    "# unmasker2b = pipeline('fill-mask', tokenizer=tokenizer_extended, model=model2b, device=device)\n",
    "#\n",
    "# unmasker0a = pipeline('fill-mask', tokenizer='bert-base-uncased', model=model0a, device=device)\n",
    "# unmasker0b = pipeline('fill-mask', tokenizer='distilbert-base-uncased', model=model0b, device=device)\n",
    "\n",
    "# whole_word_unmasker = pipeline('fill-mask', model='bert-large-uncased-whole-word-masking', device=device)\n",
    "base_unmasker = pipeline('fill-mask', model='bert-base-uncased', device=device)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-11T23:20:45.490579Z",
     "start_time": "2023-06-11T23:20:02.006520Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [],
   "source": [
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def lemmatize_a_word(word):\n",
    "    split_word = word.split()\n",
    "    # print(split_word)\n",
    "    ngram=len(split_word)\n",
    "    if ngram == 1:\n",
    "        return lemmatizer.lemmatize(word, pos='n')\n",
    "    else:\n",
    "        last_word =split_word[-1]\n",
    "        lem = lemmatizer.lemmatize(last_word, pos='n')\n",
    "        new_word = ' '.join(split_word[:-1] + [lem])\n",
    "        return new_word"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-11T23:30:33.841115Z",
     "start_time": "2023-06-11T23:30:33.822585Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Customisable RaTE patterns\n",
    "(add more descriptions)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [],
   "source": [
    "# paper page 5\n",
    "def generate_queries_paper(token):\n",
    "    token = lemmatize_a_word(token) # lemmatize to remove inflection\n",
    "\n",
    "    p1a = f\" {token} [MASK] .\"\n",
    "    p1b = f\" [MASK] {token} .\"\n",
    "\n",
    "    p2a = f\" {token} is a [MASK] .\"\n",
    "    p2b = f\" {token} is an [MASK] .\"\n",
    "\n",
    "    p3a = f\" {token} is a kind of [MASK] .\"\n",
    "    p3b = f\" {token} is a type of [MASK] .\"\n",
    "    p3c = f\" {token} is an example of [MASK] .\"\n",
    "\n",
    "    p4a = f\" [MASK] such as {token} .\"\n",
    "    p4b = f\" A [MASK] such as {token} .\"\n",
    "    p4c = f\" An [MASK] such as {token} .\"\n",
    "\n",
    "    p5a = f\" My favorite [MASK] is {token} .\"\n",
    "\n",
    "    # read customized patterns from filestream?\n",
    "    # ...\n",
    "\n",
    "    return [p1a, p1b,\n",
    "            p2a, p2b,\n",
    "            p3a, p3b, p3c,\n",
    "            p4a, p4b, p4c,\n",
    "            p5a]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-12T01:06:21.680920Z",
     "start_time": "2023-06-12T01:06:21.665300Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "outputs": [],
   "source": [
    "def generate_queries_from_template(token):\n",
    "    test_queries = []\n",
    "\n",
    "    for template in query_templates:\n",
    "        test_queries.append(template.replace(\"{token}\", token) + \" .\")\n",
    "\n",
    "    return test_queries"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-12T02:01:24.792168Z",
     "start_time": "2023-06-12T02:01:24.790352Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 524/524 [01:40<00:00,  5.22it/s]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "for predictor in [base_unmasker]:\n",
    "    predictions = {}\n",
    "\n",
    "    # get 1e results\n",
    "    for child in tqdm(eval_children):\n",
    "        queries = generate_queries_from_template(child)\n",
    "        queries_unmasked = base_unmasker(queries, top_k=top_k)\n",
    "        for i, column in enumerate(column_names):\n",
    "            if child not in predictions:\n",
    "                predictions[child] = {}\n",
    "            predictions[child][column] = queries_unmasked[i]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-12T02:25:20.820708Z",
     "start_time": "2023-06-12T02:23:40.414437Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "outputs": [
    {
     "data": {
      "text/plain": "[{'score': 0.10947363078594208,\n  'token': 14638,\n  'token_str': 'perennial',\n  'sequence': 'mussel is a perennial.'},\n {'score': 0.0480949766933918,\n  'token': 17901,\n  'token_str': 'weed',\n  'sequence': 'mussel is a weed.'},\n {'score': 0.02520201914012432,\n  'token': 23566,\n  'token_str': 'vegetarian',\n  'sequence': 'mussel is a vegetarian.'},\n {'score': 0.02241355925798416,\n  'token': 24024,\n  'token_str': 'bacterium',\n  'sequence': 'mussel is a bacterium.'},\n {'score': 0.022051049396395683,\n  'token': 26835,\n  'token_str': 'pathogen',\n  'sequence': 'mussel is a pathogen.'},\n {'score': 0.01925797015428543,\n  'token': 15267,\n  'token_str': 'predator',\n  'sequence': 'mussel is a predator.'},\n {'score': 0.019203638657927513,\n  'token': 25742,\n  'token_str': 'sponge',\n  'sequence': 'mussel is a sponge.'},\n {'score': 0.017182761803269386,\n  'token': 12573,\n  'token_str': 'freshwater',\n  'sequence': 'mussel is a freshwater.'},\n {'score': 0.015674659982323647,\n  'token': 3269,\n  'token_str': 'plant',\n  'sequence': 'mussel is a plant.'},\n {'score': 0.015475239604711533,\n  'token': 3869,\n  'token_str': 'fish',\n  'sequence': 'mussel is a fish.'}]"
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions[\"mussel\"]['is a']"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-12T03:20:03.245716Z",
     "start_time": "2023-06-12T03:20:03.221750Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 11/11 [00:00<00:00, 710.46it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": "    hypernym         hyponym  {token} [MASK]  [MASK] {token}  is a  is an  \\\n0    seafood          mussel               0               0     0      0   \n1    seafood            clam               0               0     0      0   \n2    seafood         lobster               0               0     0      0   \n3    seafood          oyster               0               0     0      0   \n4    seafood          shrimp               0               0     0      0   \n..       ...             ...             ...             ...   ...    ...   \n519    music      jazz music               0               0     0      0   \n520    music            jazz               0               0     0      0   \n521    music    upbeat music               1               0     0      0   \n522    music      front desk               0               0     0      0   \n523    music  regina spektor               0               0     0      0   \n\n     kind of  type of  example of  such as  a such as  an such as  favorite  \\\n0          0        0           0        1          1           0         1   \n1          0        0           0        1          0           0         0   \n2          0        1           0        1          0           1         1   \n3          0        1           0        1          0           0         1   \n4          0        1           0        1          0           0         1   \n..       ...      ...         ...      ...        ...         ...       ...   \n519        1        1           1        1          1           0         1   \n520        1        1           1        1          1           0         1   \n521        1        1           1        1          1           0         1   \n522        0        0           0        0          0           0         0   \n523        0        0           0        0          0           0         0   \n\n     sum  \n0      3  \n1      1  \n2      4  \n3      3  \n4      3  \n..   ...  \n519    6  \n520    6  \n521    7  \n522    0  \n523    0  \n\n[524 rows x 14 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>hypernym</th>\n      <th>hyponym</th>\n      <th>{token} [MASK]</th>\n      <th>[MASK] {token}</th>\n      <th>is a</th>\n      <th>is an</th>\n      <th>kind of</th>\n      <th>type of</th>\n      <th>example of</th>\n      <th>such as</th>\n      <th>a such as</th>\n      <th>an such as</th>\n      <th>favorite</th>\n      <th>sum</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>seafood</td>\n      <td>mussel</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>1</td>\n      <td>0</td>\n      <td>1</td>\n      <td>3</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>seafood</td>\n      <td>clam</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>seafood</td>\n      <td>lobster</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>1</td>\n      <td>1</td>\n      <td>4</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>seafood</td>\n      <td>oyster</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>3</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>seafood</td>\n      <td>shrimp</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>3</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>519</th>\n      <td>music</td>\n      <td>jazz music</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>0</td>\n      <td>1</td>\n      <td>6</td>\n    </tr>\n    <tr>\n      <th>520</th>\n      <td>music</td>\n      <td>jazz</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>0</td>\n      <td>1</td>\n      <td>6</td>\n    </tr>\n    <tr>\n      <th>521</th>\n      <td>music</td>\n      <td>upbeat music</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>0</td>\n      <td>1</td>\n      <td>7</td>\n    </tr>\n    <tr>\n      <th>522</th>\n      <td>music</td>\n      <td>front desk</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>523</th>\n      <td>music</td>\n      <td>regina spektor</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n<p>524 rows × 14 columns</p>\n</div>"
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df_result = pd.DataFrame()\n",
    "df_result[\"hypernym\"] = eval_parents\n",
    "df_result[\"hyponym\"] = eval_children\n",
    "\n",
    "for column_name in tqdm(column_names):\n",
    "    query_predictions = []\n",
    "\n",
    "    for parent, child in zip(eval_parents, eval_children):\n",
    "        preds = predictions[child][column_name]\n",
    "\n",
    "        parent_in_preds = False\n",
    "\n",
    "        for pred_list in preds:\n",
    "            if parent == pred_list['token_str']:\n",
    "                parent_in_preds = True\n",
    "                break\n",
    "\n",
    "\n",
    "        query_predictions.append(parent_in_preds * 1)\n",
    "\n",
    "    df_result[column_name] = query_predictions\n",
    "\n",
    "df_result['sum'] = df_result[column_names].sum(axis=1)\n",
    "df_result"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-12T04:20:25.822922Z",
     "start_time": "2023-06-12T04:20:25.778858Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'>' not supported between instances of 'method' and 'int'",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mTypeError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[57], line 1\u001B[0m\n\u001B[0;32m----> 1\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;28mlen\u001B[39m(df_result\u001B[38;5;241m.\u001B[39mloc[\u001B[43mdf_result\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msum\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m>\u001B[39;49m\u001B[43m \u001B[49m\u001B[38;5;241;43m0\u001B[39;49m]) \u001B[38;5;241m/\u001B[39m \u001B[38;5;28mlen\u001B[39m(df_result))\n",
      "\u001B[0;31mTypeError\u001B[0m: '>' not supported between instances of 'method' and 'int'"
     ]
    }
   ],
   "source": [
    "print(len(df_result.loc[df_result[\"sum\"] > 0]) / len(df_result))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-12T04:24:47.578613Z",
     "start_time": "2023-06-12T04:24:47.571808Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Find the best matching guesses for the masked word\n",
    "text: a sentence much include exactly one [MASK] token to predict.\n",
    "      For example:\n",
    "      'Alex likes to have [MASK] with his best friend'\n",
    "model: a BertForMaskedLM\n",
    "tokenizer: a Bert tokenizer\n",
    "topn: Number of candidates for mask\n",
    "\n",
    "Returns candidates and their probs\n",
    "\"\"\"\n",
    "\n",
    "# ref: ...\n",
    "\n",
    "# def predict_word(text, model, tokenizer, topn=10):\n",
    "def predict_word(text, model, tokenizer, distil=False):\n",
    "    # Prepare tex\n",
    "    text = '[CLS] '+ text.lstrip('[CLS] ').rstrip(' [SEP]')+' [SEP]'\n",
    "    # Tokenize input\n",
    "    tokenized_text = tokenizer.tokenize(text)\n",
    "\n",
    "    # Mask a token that we will try to predict back with `BertForMaskedLM`\n",
    "    masked_index = -1\n",
    "    for i, token in enumerate(tokenized_text):\n",
    "        if token=='[MASK]':\n",
    "            masked_index = i\n",
    "            break\n",
    "    assert i>=0\n",
    "\n",
    "    # Convert token to vocabulary indices\n",
    "    indexed_tokens = tokenizer.convert_tokens_to_ids(tokenized_text)\n",
    "    # Define sentence A and B indices associated to 1st and 2nd sentences (see paper)\n",
    "    segments_ids = [0]*len(tokenized_text)\n",
    "\n",
    "    # Convert inputs to PyTorch tensors\n",
    "    tokens_tensor = torch.tensor([indexed_tokens])\n",
    "    segments_tensors = torch.tensor([segments_ids])\n",
    "\n",
    "    # If you have a GPU, put everything on cuda\n",
    "    tokens_tensor = tokens_tensor.to(device)\n",
    "    if not distil:\n",
    "        segments_tensors = segments_tensors.to(device)\n",
    "\n",
    "    # Predict all tokens\n",
    "    with torch.no_grad():\n",
    "        if distil:\n",
    "            outputs = model(tokens_tensor)\n",
    "        else:\n",
    "            outputs = model(tokens_tensor, token_type_ids=segments_tensors)\n",
    "        predictions = outputs[0]\n",
    "    # print(\"Predictions shape: \" + str(predictions[0].shape))\n",
    "    predicted_inds = torch.argsort(-predictions[0, masked_index])\n",
    "    # print(predicted_inds[:topn])\n",
    "    predicted_probs = [round(p.item(),4) for p in torch.softmax(predictions[0, masked_index], 0)[predicted_inds]]\n",
    "    predicted_tokens = tokenizer.convert_ids_to_tokens([ind.item() for ind in predicted_inds])\n",
    "\n",
    "    # return list(zip(predicted_tokens, predicted_probs))[:topn]\n",
    "    return list(zip(predicted_tokens, predicted_probs)), dict(list(zip(predicted_tokens, predicted_probs)))\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
