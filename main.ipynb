{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# RaTE: a Reproducible automatic Taxonomy Evaluation by Filling the Gap\n",
    "\n",
    "**Input:** a taxonomy in the form of concept pairs\n",
    "**Output:** a score associated with the input generated by a large masked language model"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "# from transformers import BertTokenizer, BertForMaskedLM, DistilBertForMaskedLM\n",
    "from transformers import BertTokenizer, BertForMaskedLM\n",
    "from transformers import pipeline\n",
    "import nltk\n",
    "# nltk.download('wordnet') // required for lemmatizer\n",
    "# nltk.download('omw-1.4') // required for lemmatizer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import torch\n",
    "import pandas as pd"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-13T01:56:17.056664Z",
     "start_time": "2023-06-13T01:56:14.699021Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# All the hyperparameters to control the flow of RaTE"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "# parameters\n",
    "top_k = 10  # specifies the number of predictions generated by LLM per query\n",
    "eval_taxo_path = \"taxos/HiExpan1.txt\"  # indicates the path to the candidate taxonomy for evaluation\n",
    "use_queries = \"custom_queries/default_queries.txt\""
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-13T01:56:19.251485Z",
     "start_time": "2023-06-13T01:56:19.241646Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('{token} [MASK]', '{token} [MASK]'), ('[MASK] {token}', '[MASK] {token}'), ('{token} is a [MASK]', 'is a'), ('{token} is an [MASK]', 'is an'), ('{token} is a kind of [MASK]', 'kind of'), ('{token} is a type of [MASK]', 'type of'), ('{token} is an example of [MASK]', 'example of'), ('[MASK] such as {token}', 'such as'), ('A [MASK] such as {token}', 'a such as'), ('An [MASK] such as {token}', 'an such as'), ('My favorite [MASK] is {token}', 'favorite')]\n"
     ]
    }
   ],
   "source": [
    "query_templates = []\n",
    "column_names = []\n",
    "\n",
    "with open(use_queries, \"r\") as fin:\n",
    "    for line in fin.readlines():\n",
    "        if not line.startswith(\"#\"):\n",
    "            query, name = line.split(\",\")\n",
    "            query_templates.append(query.strip())\n",
    "            column_names.append(name.strip())\n",
    "\n",
    "print(list(zip(query_templates, column_names)))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-13T01:56:20.700241Z",
     "start_time": "2023-06-13T01:56:20.692914Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "eval_parents = []\n",
    "eval_children = []\n",
    "\n",
    "with open(eval_taxo_path , \"r\") as fin:\n",
    "    for concept_pair in fin.readlines():\n",
    "        parent, child = concept_pair.split(',')\n",
    "        eval_parents.append(parent.strip())\n",
    "        eval_children.append(child.strip())"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-13T01:56:22.272312Z",
     "start_time": "2023-06-13T01:56:22.264568Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "data": {
      "text/plain": "{'veggies': ['veggie', 'vegetable', 'vegetables'], 'dessert': ['desert']}"
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "alt_accepted_answers = {}\n",
    "\n",
    "with open(\"results/acceptable_alternative_answers.txt\", \"r\") as fin:\n",
    "    for line in fin.readlines():\n",
    "        taxo_ent, alts = line.split(\":\")\n",
    "        taxo_ent = taxo_ent.strip()\n",
    "        alts = [x.strip() for x in alts.split(\",\")]\n",
    "        alt_accepted_answers[taxo_ent] = alts\n",
    "alt_accepted_answers"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-13T02:04:46.838888Z",
     "start_time": "2023-06-13T02:04:46.828349Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "data": {
      "text/plain": "31"
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# instantiate a standard bert-base tokenizer\n",
    "bert_base_tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "# instantiate another tokenizer for adding customized tokens\n",
    "bert_base_extended = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "not_in_bert_parent_lemmas = []\n",
    "\n",
    "with open(\"model_configuration/add_tokenizer_vocabulary.txt\", \"r\") as fin:\n",
    "    for word in fin.readlines():\n",
    "        not_in_bert_parent_lemmas.append(word.strip())\n",
    "\n",
    "bert_base_extended.add_tokens(not_in_bert_parent_lemmas)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-13T02:05:15.151859Z",
     "start_time": "2023-06-13T02:05:14.974963Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "# can also use mps on mac\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "elif torch.backends.mps.is_available() and torch.backends.mps.is_built():\n",
    "    device = torch.device('mps')\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "\n",
    "# load models\n",
    "# model1a = BertForMaskedLM.from_pretrained('/content/drive/MyDrive/mlm_final_exps/entity_masking_train_test/checkpoint-117350')\n",
    "# model1a.to(device)\n",
    "# model1a.eval()\n",
    "#\n",
    "# model1b = BertForMaskedLM.from_pretrained('/content/drive/MyDrive/mlm_final_exps/entity_masking_train_test_only_one/final/checkpoint-58675')\n",
    "# model1b.to(device)\n",
    "# model1b.eval()\n",
    "#\n",
    "# model2a = BertForMaskedLM.from_pretrained('/content/drive/MyDrive/mlm_final_exps/random_masking_train_test/final/checkpoint-58675')\n",
    "# model2a.to(device)\n",
    "# model2a.eval()\n",
    "#\n",
    "# model2b = BertForMaskedLM.from_pretrained('/content/drive/MyDrive/mlm_final_exps/random_masking_train_only/checkpoint-82146')\n",
    "# model2b.to(device)\n",
    "# model2b.eval()\n",
    "#\n",
    "# model0a = BertForMaskedLM.from_pretrained('/content/drive/MyDrive/exp/yelp/out_random_masking-bert/checkpoint-80000')\n",
    "# model0a.to(device)\n",
    "# model0a.eval()\n",
    "#\n",
    "# model0b = DistilBertForMaskedLM.from_pretrained('/content/drive/MyDrive/exp/yelp/out-entity-masking/checkpoint-73675')\n",
    "# model0b.to(device)\n",
    "# model0b.eval()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-13T02:05:17.171524Z",
     "start_time": "2023-06-13T02:05:17.168261Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "data": {
      "text/plain": "Downloading (…)lve/main/config.json:   0%|          | 0.00/728 [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "426f40292bd84f2086e1a92ef01f7bd5"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Downloading pytorch_model.bin:   0%|          | 0.00/438M [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "add1ef4e85e14543bf101dc3b4407ee1"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Downloading (…)neration_config.json:   0%|          | 0.00/90.0 [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "0a4874ef5e994238aeb199cf1ae641f2"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# or, just use pipeline\n",
    "# !pip install transformers\n",
    "# load a single model or multiple\n",
    "\n",
    "unmasker1a = pipeline('fill-mask', tokenizer=bert_base_extended, model=\"RaTE-Paper/yelp-m1a\", device=device)\n",
    "# unmasker1b = pipeline('fill-mask', tokenizer=bert_base_extended, model=\"RaTE-Paper/yelp-m1b\", device=device)\n",
    "# unmasker2a = pipeline('fill-mask', tokenizer=bert_base_extended, model=\"RaTE-Paper/yelp-m2a\", device=device)\n",
    "# unmasker2b = pipeline('fill-mask', tokenizer=bert_base_extended, model=\"RaTE-Paper/yelp-m2b\", device=device)\n",
    "#\n",
    "# unmasker0a = pipeline('fill-mask', tokenizer='bert-base-uncased', model=\"RaTE-Paper/yelp-m0a\", device=device)\n",
    "# unmasker0b = pipeline('fill-mask', tokenizer='distilbert-base-uncased', model=\"RaTE-Paper/yelp-m0b\", device=device)\n",
    "#\n",
    "# whole_word_unmasker = pipeline('fill-mask', model='bert-large-uncased-whole-word-masking', device=device)\n",
    "# base_unmasker = pipeline('fill-mask', model='bert-base-uncased', device=device)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-13T02:06:06.814817Z",
     "start_time": "2023-06-13T02:05:25.335056Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [],
   "source": [
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def lemmatize_a_word(word):\n",
    "    split_word = word.split()\n",
    "    # print(split_word)\n",
    "    ngram=len(split_word)\n",
    "    if ngram == 1:\n",
    "        return lemmatizer.lemmatize(word, pos='n')\n",
    "    else:\n",
    "        last_word =split_word[-1]\n",
    "        lem = lemmatizer.lemmatize(last_word, pos='n')\n",
    "        new_word = ' '.join(split_word[:-1] + [lem])\n",
    "        return new_word"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-13T02:07:07.636325Z",
     "start_time": "2023-06-13T02:07:07.633450Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Customisable RaTE patterns\n",
    "(add more descriptions)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [],
   "source": [
    "# paper page 5\n",
    "def generate_queries_paper(token):\n",
    "    token = lemmatize_a_word(token) # lemmatize to remove inflection\n",
    "\n",
    "    p1a = f\" {token} [MASK] .\"\n",
    "    p1b = f\" [MASK] {token} .\"\n",
    "\n",
    "    p2a = f\" {token} is a [MASK] .\"\n",
    "    p2b = f\" {token} is an [MASK] .\"\n",
    "\n",
    "    p3a = f\" {token} is a kind of [MASK] .\"\n",
    "    p3b = f\" {token} is a type of [MASK] .\"\n",
    "    p3c = f\" {token} is an example of [MASK] .\"\n",
    "\n",
    "    p4a = f\" [MASK] such as {token} .\"\n",
    "    p4b = f\" A [MASK] such as {token} .\"\n",
    "    p4c = f\" An [MASK] such as {token} .\"\n",
    "\n",
    "    p5a = f\" My favorite [MASK] is {token} .\"\n",
    "\n",
    "    # read customized patterns from filestream?\n",
    "    # ...\n",
    "\n",
    "    return [p1a, p1b,\n",
    "            p2a, p2b,\n",
    "            p3a, p3b, p3c,\n",
    "            p4a, p4b, p4c,\n",
    "            p5a]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-13T02:07:12.881416Z",
     "start_time": "2023-06-13T02:07:12.876228Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [],
   "source": [
    "def generate_queries_from_template(token):\n",
    "    test_queries = []\n",
    "\n",
    "    for template in query_templates:\n",
    "        test_queries.append(template.replace(\"{token}\", token) + \" .\")\n",
    "\n",
    "    return test_queries"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-13T02:07:14.824980Z",
     "start_time": "2023-06-13T02:07:14.817695Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 524/524 [01:45<00:00,  4.95it/s]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "use_eval_models = [unmasker1a]\n",
    "\n",
    "\n",
    "for predictor in use_eval_models:\n",
    "    predictions = {}\n",
    "\n",
    "    # get 1e results\n",
    "    for child in tqdm(eval_children):\n",
    "        queries = generate_queries_from_template(child)\n",
    "        # queries = generate_queries_paper(child)\n",
    "        queries_unmasked = predictor(queries, top_k=top_k)\n",
    "        for i, column in enumerate(column_names):\n",
    "            if child not in predictions:\n",
    "                predictions[child] = {}\n",
    "            predictions[child][column] = queries_unmasked[i]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-13T02:25:26.189128Z",
     "start_time": "2023-06-13T02:23:40.212736Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 11/11 [00:00<00:00, 821.87it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": "    hypernym         hyponym  {token} [MASK]  [MASK] {token}  is a  is an  \\\n0    seafood          mussel               0               1     0      0   \n1    seafood            clam               0               0     0      0   \n2    seafood         lobster               0               0     0      0   \n3    seafood          oyster               0               0     0      0   \n4    seafood          shrimp               0               0     0      0   \n..       ...             ...             ...             ...   ...    ...   \n519    music      jazz music               1               0     0      0   \n520    music            jazz               1               0     0      0   \n521    music    upbeat music               1               0     0      0   \n522    music      front desk               0               0     0      0   \n523    music  regina spektor               0               0     0      0   \n\n     kind of  type of  example of  such as  a such as  an such as  favorite  \\\n0          1        1           1        1          1           0         1   \n1          0        0           0        1          0           0         0   \n2          1        1           1        1          1           1         1   \n3          1        1           1        1          1           1         1   \n4          1        1           1        1          1           1         1   \n..       ...      ...         ...      ...        ...         ...       ...   \n519        1        1           1        1          1           0         1   \n520        1        1           1        1          1           1         1   \n521        1        1           1        1          1           0         1   \n522        0        0           0        0          0           0         0   \n523        0        0           0        0          0           0         0   \n\n     sum  \n0      7  \n1      1  \n2      7  \n3      7  \n4      7  \n..   ...  \n519    7  \n520    8  \n521    7  \n522    0  \n523    0  \n\n[524 rows x 14 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>hypernym</th>\n      <th>hyponym</th>\n      <th>{token} [MASK]</th>\n      <th>[MASK] {token}</th>\n      <th>is a</th>\n      <th>is an</th>\n      <th>kind of</th>\n      <th>type of</th>\n      <th>example of</th>\n      <th>such as</th>\n      <th>a such as</th>\n      <th>an such as</th>\n      <th>favorite</th>\n      <th>sum</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>seafood</td>\n      <td>mussel</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>0</td>\n      <td>1</td>\n      <td>7</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>seafood</td>\n      <td>clam</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>seafood</td>\n      <td>lobster</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>7</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>seafood</td>\n      <td>oyster</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>7</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>seafood</td>\n      <td>shrimp</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>7</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>519</th>\n      <td>music</td>\n      <td>jazz music</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>0</td>\n      <td>1</td>\n      <td>7</td>\n    </tr>\n    <tr>\n      <th>520</th>\n      <td>music</td>\n      <td>jazz</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>8</td>\n    </tr>\n    <tr>\n      <th>521</th>\n      <td>music</td>\n      <td>upbeat music</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>0</td>\n      <td>1</td>\n      <td>7</td>\n    </tr>\n    <tr>\n      <th>522</th>\n      <td>music</td>\n      <td>front desk</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>523</th>\n      <td>music</td>\n      <td>regina spektor</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n<p>524 rows × 14 columns</p>\n</div>"
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def generate_result_frame(accept_noisy_answers=False):\n",
    "    df_result = pd.DataFrame()\n",
    "    df_result[\"hypernym\"] = eval_parents\n",
    "    df_result[\"hyponym\"] = eval_children\n",
    "\n",
    "    for column_name in tqdm(column_names):\n",
    "        query_predictions = []\n",
    "\n",
    "        for parent, child in zip(eval_parents, eval_children):\n",
    "            preds = predictions[child][column_name]\n",
    "\n",
    "            parent_in_preds = False\n",
    "\n",
    "            for pred_list in preds:\n",
    "                if parent == pred_list['token_str']:\n",
    "                    parent_in_preds = True\n",
    "                    break\n",
    "                if accept_noisy_answers:\n",
    "                    if parent in alt_accepted_answers:\n",
    "                        if pred_list['token_str'] in alt_accepted_answers[parent]:\n",
    "                            parent_in_preds = True\n",
    "                            break\n",
    "            query_predictions.append(parent_in_preds * 1)\n",
    "\n",
    "        df_result[column_name] = query_predictions\n",
    "\n",
    "    df_result['sum'] = df_result[column_names].sum(axis=1)\n",
    "    return df_result\n",
    "\n",
    "df_result = generate_result_frame(accept_noisy_answers=True)\n",
    "df_result"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-13T03:15:02.280227Z",
     "start_time": "2023-06-13T03:15:02.254214Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Generate and display RaTE score"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RaTE score: 0.8454198473282443\n"
     ]
    }
   ],
   "source": [
    "print(\"RaTE score:\", len(df_result.loc[df_result[\"sum\"] > 0]) / len(df_result))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-13T03:15:54.131241Z",
     "start_time": "2023-06-13T03:15:54.126014Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# additional code for predicting a masked token in the absence of unmaskers\n",
    "# ref: https://gist.github.com/yuchenlin/a2f42d3c4378ed7b83de65c7a2222eb2\n",
    "\n",
    "# def predict_word(text, model, tokenizer, topn=10):\n",
    "def predict_word(text, model, tokenizer, distil=False):\n",
    "    # Prepare tex\n",
    "    text = '[CLS] '+ text.lstrip('[CLS] ').rstrip(' [SEP]')+' [SEP]'\n",
    "    # Tokenize input\n",
    "    tokenized_text = tokenizer.tokenize(text)\n",
    "\n",
    "    # Mask a token that we will try to predict back with `BertForMaskedLM`\n",
    "    masked_index = -1\n",
    "    for i, token in enumerate(tokenized_text):\n",
    "        if token=='[MASK]':\n",
    "            masked_index = i\n",
    "            break\n",
    "    assert i>=0\n",
    "\n",
    "    # Convert token to vocabulary indices\n",
    "    indexed_tokens = tokenizer.convert_tokens_to_ids(tokenized_text)\n",
    "    # Define sentence A and B indices associated to 1st and 2nd sentences (see paper)\n",
    "    segments_ids = [0]*len(tokenized_text)\n",
    "\n",
    "    # Convert inputs to PyTorch tensors\n",
    "    tokens_tensor = torch.tensor([indexed_tokens])\n",
    "    segments_tensors = torch.tensor([segments_ids])\n",
    "\n",
    "    # If you have a GPU, put everything on cuda\n",
    "    tokens_tensor = tokens_tensor.to(device)\n",
    "    if not distil:\n",
    "        segments_tensors = segments_tensors.to(device)\n",
    "\n",
    "    # Predict all tokens\n",
    "    with torch.no_grad():\n",
    "        if distil:\n",
    "            outputs = model(tokens_tensor)\n",
    "        else:\n",
    "            outputs = model(tokens_tensor, token_type_ids=segments_tensors)\n",
    "        predictions = outputs[0]\n",
    "\n",
    "    # print(\"Predictions shape: \" + str(predictions[0].shape))\n",
    "    predicted_inds = torch.argsort(-predictions[0, masked_index])\n",
    "    # print(predicted_inds[:topn])\n",
    "    predicted_probs = [round(p.item(),4) for p in torch.softmax(predictions[0, masked_index], 0)[predicted_inds]]\n",
    "    predicted_tokens = tokenizer.convert_ids_to_tokens([ind.item() for ind in predicted_inds])\n",
    "\n",
    "    # return list(zip(predicted_tokens, predicted_probs))[:topn]\n",
    "    return list(zip(predicted_tokens, predicted_probs)), dict(list(zip(predicted_tokens, predicted_probs)))"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
